\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2.2cm,lmargin=2.5cm,rmargin=2.5cm}

\usepackage{url}
\usepackage{breakurl}
\usepackage[parfill]{parskip}
\usepackage{amsmath}


\begin{document}
\begin{center}
\section*{Win Forecasting Through Predictive Modeling}
by Travis Byrum
\end{center}
\paragraph
\\
The most important purpose of NBA analytics is player evaluation since player performance is the deciding factor behind team wins.  Determining a player's contribution to team performance is difficult due to basketball's team-oriented nature and as a result there is an great deal of debate on the subject.  A common approach is to use plus/minus metrics as means of evaluating a player's isolated production.  The idea behind plus/minus is to use the net difference in score of a player on versus off the floor as a an indicator of that player's contribution to team performance.  Others have noted that using the pure point differential would be a poor metric in determining a player's impact on team performance since it is heavily influenced by the other players on the court.  This problem led to the creation of other metrics such such as $\textit{APM}$ (Adjusted Plus Minus) which attempts to reduce bias by taking into account teammate and opponent level of talent.  One of the most recent versions of the plus/minus metric is $\textit{RAPM}$ (Regularized Adjusted Plus Minus) which was developed by analyst Jeremias Engelmann as a means of reducing the standard errors of win predictions associated with $\textit{APM}$.  This is accomplished through the use of ridge regression which penalizes unreasonable parameter estimates.  The flexibility of $\textit{RAPM}$ is what makes it so compelling.  The rating can be used to not only evaluate an individual player but also can be combined to forecast lineups and team wins.
\paragraph
\\
The purpose of this article is to attempt to outperform  $\textit{RAPM}$ through the use of predictive modeling.   Although plus/minus approaches are an extremely useful heuristic, it is possible to achieve greater accuracy in forecasting team wins.  To accomplish this, I decided to take a lineup-based approach where the stats from each individual player in the lineup function as the covariates and the lineup-associated wins would be the dependent variable.  In order to take advantage of the wide variety of data types associated with NBA statistics, I used regression decision trees due to their flexibility in modeling interactions.  More specifically, I used an ensemble learning technique known as gradient boosting (also known as $\textit{GBM}$) .  The idea behind gradient boosting is to combine several smaller decision tree models each with individually weak predictive performance, into a larger model with greater performance.  It can be understood as an an additive model \footnote{http://en.wikipedia.org/wiki/Gradient$\textunderscore$boosting} as seen in the following equation:
\begin{equation}
F(x) = \sum_{i=1}^M \gamma_i h_i(x) + \mbox{const}.
\end{equation}
In this context, each term $h_i(x)$ represents an individual regression decision tree and $\gamma_i$ is a weight calculated through resampling .  Although the specifics as to how the weights and terms are derived is a bit complicated, the advantage of this approach is greater predictive accuracy and flexibility than would be achievable with standard OLS regression.
\paragraph
\\
To apply this approach, I decided to fit the model on lineup data from the 2014-2015 season\footnote{Data gathered from Basketball-Reference}.  An individual row of the data includes information on specific lineup combinations, including games/minutes played and net points.  This can be seen in the following output. 
\\
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Parsed with column specification:\\\#\# cols(\\\#\#\ \  .default = col\_double(),\\\#\#\ \  Rk = col\_integer(),\\\#\#\ \  Lineup = col\_character(),\\\#\#\ \  Tm = col\_character(),\\\#\#\ \  Season = col\_character(),\\\#\#\ \  G = col\_integer(),\\\#\#\ \  Tm\_poss = col\_integer(),\\\#\#\ \  Opp\_poss = col\_integer(),\\\#\#\ \  FG = col\_integer(),\\\#\#\ \  FGA = col\_integer(),\\\#\#\ \  X3P = col\_integer(),\\\#\#\ \  X3PA = col\_integer(),\\\#\#\ \  FT = col\_integer(),\\\#\#\ \  FTA = col\_integer(),\\\#\#\ \  PTS = col\_integer(),\\\#\#\ \  FG\_opp = col\_integer(),\\\#\#\ \  FGA\_opp = col\_integer(),\\\#\#\ \  X3P\_opp = col\_integer(),\\\#\#\ \  X3PA\_opp = col\_integer(),\\\#\#\ \  FT\_opp = col\_integer(),\\\#\#\ \  FTA\_opp = col\_integer()\\\#\#\ \  \# ... with 132 more columns\\\#\# )}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# See spec(...) for full column specifications.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Parsed with column specification:\\\#\# cols(\\\#\#\ \  .default = col\_integer(),\\\#\#\ \  Lineup = col\_character(),\\\#\#\ \  Tm = col\_character(),\\\#\#\ \  Season = col\_character(),\\\#\#\ \  MP = col\_double(),\\\#\#\ \  Pace = col\_double(),\\\#\#\ \  FGper = col\_double(),\\\#\#\ \  X3Pper = col\_double(),\\\#\#\ \  eFGper = col\_double(),\\\#\#\ \  FTper = col\_double(),\\\#\#\ \  FGper\_opp = col\_double(),\\\#\#\ \  X3Pper\_opp = col\_double(),\\\#\#\ \  eFGper\_opp = col\_double(),\\\#\#\ \  FTper\_opp = col\_double()\\\#\# )}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# See spec(...) for full column specifications.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Parsed with column specification:\\\#\# cols(\\\#\#\ \  win\_expectation = col\_double()\\\#\# )}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(expr, envir, enclos): wrong result size (4900), expected 1000 or 1}}\begin{verbatim}
## # A tibble: 1 × 31
##      Rk                                                   Lineup    Tm
##   <int>                                                    <chr> <chr>
## 1     1 M. Barnes | B. Griffin | D. Jordan | C. Paul | J. Redick   LAC
## # ... with 28 more variables: Season <chr>, G <int>, MP <dbl>,
## #   Tm_poss <int>, Opp_poss <int>, Pace <dbl>, FG <int>, FGA <int>,
## #   FGper <dbl>, X3P <int>, X3PA <int>, X3Pper <dbl>, eFGper <dbl>,
## #   FT <int>, FTA <int>, FTper <dbl>, PTS <int>, FG_opp <int>,
## #   FGA_opp <int>, FGper_opp <dbl>, X3P_opp <int>, X3PA_opp <int>,
## #   X3Pper_opp <dbl>, eFGper_opp <dbl>, FT_opp <int>, FTA_opp <int>,
## #   FTper_opp <dbl>, PTS_opp <int>
\end{verbatim}
\end{kframe}
\end{knitrout}

\paragraph
\\
The main point of interest is the net points associated with each lineup since this is what largely determines team wins.  I decided to project the net points into win percentages using the formula\footnote{http://statitudes.com/blog/2013/09/09/pythagoras-of-the-hardwood/}:
\begin{equation}
Win\% = \frac{\text{points for}^{13.91}}{\text{points for}^{13.91} + \text{points against}^{13.91}}
\end{equation}
\paragraph
\\
This projection is known as the Pythagorean win expectation.  It can be thought of as the winning percentage of a lineup if they were playing $100\%$ of the team's minutes.  Although we are modeling a wins projection for interpretability, this is the same as modeling the point differential per lineup and then applying a transformation.  Intuitively since we think of lineup production as a function of the individual players' production, I decided to use the individual player stats as predictors.  With this in mind the model formula becomes
\begin{equation}
E(Win\%)_{i}\sim X_{player1i}+X_{player2i}+X_{player3i}+X_{player4i}+X_{player5i}....
\end{equation}
where $i$ corresponds to the $i^{th}$ lineup and $X_{player1i}$ corresponds to a statistic for a player in lineup $i$.  I've included a number of different statistics in the model for each player including evaluation ratings such as $\textit{PER}$ and $\textit{Offensive/Defensive Rating}$.  The final result is a $365$ length vector for each lineup which includes $73$ unique performance metrics for each player.  The idea here is that we can combine information across players' average performances to predict their specific performance in a given lineup combination.  Although many of the variables are very correlated with each other, the beauty of this modeling approach is that boosted decision trees are already robust against collinearity.
\paragraph
\\
Using the previousspecification, I ran the model on $3911$ different lineup combinations from the 2014-2015 season.  This number was chosen by necessity since I did not have the necessary individual statistics for every player in every lineup.  Since the model does not make the familiar probabilistic assumptions of linear regression, there are different diagnostics to determine overfitting.  Since $\textit{GBM}$ are built from a large number of smaller decision trees, one of the most important parameters is determining the number of trees (known as the iterations) to fit the data.  The following plot gives us the squared error loss associated with increasing the number of trees:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: survival}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: lattice}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: splines}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: parallel}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loaded gbm 2.1.1}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(expr, envir, enclos): object 'wins' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in gbm.perf(gbm1, method = "{}cv"{}): object 'gbm1' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in predict(gbm1, newdata = testdf, type = "{}response"{}, n.trees = 100): object 'gbm1' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in mean((pred - winexpectation\$x[-train\_rows])\textasciicircum{}2, na.rm = TRUE): object 'pred' not found}}\end{kframe}
\end{knitrout}

