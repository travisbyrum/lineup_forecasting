\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2.2cm,lmargin=2.5cm,rmargin=2.5cm}

\usepackage{url}
\usepackage{breakurl}
\usepackage[parfill]{parskip}
\usepackage{amsmath}

<<setup, include=FALSE, cache=TRUE>>=
opts_chunk$set(fig.path='figures/', fig.align='center', fig.show='hold', cache=TRUE)
@
\begin{document}
\begin{center}
\section*{Win Forecasting Through Predictive Modeling}
by Travis Byrum
\end{center}
\paragraph
\\
The most important purpose of NBA analytics is player evaluation since player performance is the deciding factor behind team wins.  Determining a player's contribution to team performance is difficult due to basketball's team-oriented nature and as a result there is an great deal of debate on the subject.  A common approach is to use plus/minus metrics as means of evaluating a player's isolated production.  The idea behind plus/minus is to use the net difference in score of a player on versus off the floor as a an indicator of that player's contribution to team performance.  Others have noted that using the pure point differential would be a poor metric in determining a player's impact on team performance since it is heavily influenced by the other players on the court.  This problem led to the creation of other metrics such such as $\textit{APM}$ (Adjusted Plus Minus) which attempts to reduce bias by taking into account teammate and opponent level of talent.  One of the most recent versions of the plus/minus metric is $\textit{RAPM}$ (Regularized Adjusted Plus Minus) which was developed by analyst Jeremias Engelmann as a means of reducing the standard errors of win predictions associated with $\textit{APM}$.  This is accomplished through the use of ridge regression which penalizes unreasonable parameter estimates.  The flexibility of $\textit{RAPM}$ is what makes it so compelling.  The rating can be used to not only evaluate an individual player but also can be combined to forecast lineups and team wins.
\paragraph
\\
The purpose of this article is to attempt to outperform  $\textit{RAPM}$ through the use of predictive modeling.   Although plus/minus approaches are an extremely useful heuristic, it is possible to achieve greater accuracy in forecasting team wins.  To accomplish this, I decided to take a lineup-based approach where the stats from each individual player in the lineup function as the covariates and the lineup-associated wins would be the dependent variable.  In order to take advantage of the wide variety of data types associated with NBA statistics, I used regression decision trees due to their flexibility in modeling interactions.  More specifically, I used an ensemble learning technique known as gradient boosting (also known as $\textit{GBM}$) .  The idea behind gradient boosting is to combine several smaller decision tree models each with individually weak predictive performance, into a larger model with greater performance.  It can be understood as an an additive model \footnote{http://en.wikipedia.org/wiki/Gradient$\textunderscore$boosting} as seen in the following equation:
\begin{equation}
F(x) = \sum_{i=1}^M \gamma_i h_i(x) + \mbox{const}.
\end{equation}
In this context, each term $h_i(x)$ represents an individual regression decision tree and $\gamma_i$ is a weight calculated through resampling .  Although the specifics as to how the weights and terms are derived is a bit complicated, the advantage of this approach is greater predictive accuracy and flexibility than would be achievable with standard OLS regression.
\paragraph
\\
To apply this approach, I decided to fit the model on lineup data from the 2014-2015 season\footnote{Data gathered from Basketball-Reference}.  An individual row of the data includes information on specific lineup combinations, including games/minutes played and net points.  This can be seen in the following output. 
\\
<< echo=FALSE, cache=TRUE, eval=TRUE>>=
set.seed(17)
library(dplyr)

final_data <- readr::read_csv("~/lineuper/data/final_data.csv")
lineups<- readr::read_csv("~/lineuper/data/lineups.csv")
winexpectation <- readr::read_csv("~/lineuper/data/win_expectation.csv")

### calculating the win expectation as predicted by RAPM -------------------------------------------------------
df_rapm <- final_data[,!grepl("Player", colnames(final_data))]  # need to take out names for final analysis
df_rapm[is.na(df_rapm)] <- 0 # replaces missing measurements with zero since we assume NA is due to low minutes
df_rapm <- df_rapm %>%
  mutate(
    Pos   = as.factor(Pos),
    Pos.1 = as.factor(Pos.1),
    Pos.2 = as.factor(Pos.2),
    Pos.3 = as.factor(Pos.3),
    Pos.4 = as.factor(Pos.4),
    Tm    = as.factor(Tm),
    Tm.1  = as.factor(Tm.1),
    Tm.2  = as.factor(Tm.2),
    Tm.3  = as.factor(Tm.3),
    Tm.4  = as.factor(Tm.4)
  )

off <- final_data[,grepl("ORPM", colnames(final_data))] # offensive RAPM
def <- final_data[,grepl("DRPM", colnames(final_data))] # defensive RAPM
combined <- final_data %>% mutate(total = ORPM + DRPM) %>% select(total) # combined RAPM

# using wins estimated by http://statitudes.com/blog/2013/09/09/pythagoras-of-the-hardwood/
# rapm estimates

est<-rep(NA,nrow(off))
for(i in 1:nrow(off)){
  est[i]<-1/(1+exp(-0.13959*sum(combined[i,])))
}

lineups[1,]
@

\paragraph
\\
The main point of interest is the net points associated with each lineup since this is what largely determines team wins.  I decided to project the net points into win percentages using the formula\footnote{http://statitudes.com/blog/2013/09/09/pythagoras-of-the-hardwood/}:
\begin{equation}
Win\% = \frac{\text{points for}^{13.91}}{\text{points for}^{13.91} + \text{points against}^{13.91}}
\end{equation}
\paragraph
\\
This projection is known as the Pythagorean win expectation.  It can be thought of as the winning percentage of a lineup if they were playing $100\%$ of the team's minutes.  Although we are modeling a wins projection for interpretability, this is the same as modeling the point differential per lineup and then applying a transformation.  Intuitively since we think of lineup production as a function of the individual players' production, I decided to use the individual player stats as predictors.  With this in mind the model formula becomes
\begin{equation}
E(Win\%)_{i}\sim X_{player1i}+X_{player2i}+X_{player3i}+X_{player4i}+X_{player5i}....
\end{equation}
where $i$ corresponds to the $i^{th}$ lineup and $X_{player1i}$ corresponds to a statistic for a player in lineup $i$.  I've included a number of different statistics in the model for each player including evaluation ratings such as $\textit{PER}$ and $\textit{Offensive/Defensive Rating}$.  The final result is a $365$ length vector for each lineup which includes $73$ unique performance metrics for each player.  The idea here is that we can combine information across players' average performances to predict their specific performance in a given lineup combination.  Although many of the variables are very correlated with each other, the beauty of this modeling approach is that boosted decision trees are already robust against collinearity.
\paragraph
\\
Using the previousspecification, I ran the model on $3911$ different lineup combinations from the 2014-2015 season.  This number was chosen by necessity since I did not have the necessary individual statistics for every player in every lineup.  Since the model does not make the familiar probabilistic assumptions of linear regression, there are different diagnostics to determine overfitting.  Since $\textit{GBM}$ are built from a large number of smaller decision trees, one of the most important parameters is determining the number of trees (known as the iterations) to fit the data.  The following plot gives us the squared error loss associated with increasing the number of trees:

<< fig.width=4.6, fig.height=4, echo=FALSE, cache=TRUE, results='hide', eval=TRUE>>=
library(gbm)
library(dplyr)

gbm_formula <- as.formula(
  paste0("win_expectation ~ ", paste(
    setdiff(colnames(df_rapm), "win_expectation"),
    collapse = " + ")
  )
)

train_rows <- sample(nrow(df_rapm), round(nrow(df_rapm) * 0.8))
traindf <- df_rapm[train_rows, ]
testdf <- df_rapm[-train_rows, ]

#fiting the boosting algorithm
gbm1 <- gbm(
  gbm_formula,                    # formula
  data              = traindf,    # dataset
  distribution      = "gaussian", # see the help for other choices
  n.trees           = 100,        # number of trees
  shrinkage         = 0.01,       # shrinkage or learning rate,
  interaction.depth = 5,          # 1: additive model, 2: two-way interactions, etc.
  bag.fraction      = 0.5,        # subsampling fraction, 0.5 is probably best
  train.fraction    = 0.9,        # fraction of data for training,
  n.minobsinnode    = 10,         # minimum total weight needed in each node
  cv.folds          = 2,          # do n-fold cross-validation
  keep.data         = TRUE,       # keep a copy of the dataset with the object
  verbose           = TRUE,
  n.cores           = 1
)

gbm_perf <- gbm.perf(gbm1, method = "cv")
pred <- predict(
  gbm1,
  newdata = testdf,
  type    = "response",
  n.trees = 100
)

gbm_mse <- suppressWarnings(mean((pred - winexpectation$x[-train_rows])^2,na.rm = TRUE))
rapm_mse <- suppressWarnings(mean((est - winexpectation$x[-train_rows])^2,na.rm = TRUE))
@

\paragraph
\\
The squared error loss in this context is calculated from the actual response (lineup win projections calculated from point differentials) against the predicted response with a lower error being better.  The black line indicates the in sample squared error while the red and green lines indicate squared error occurring from $n$-fold cross-validation.  The dotted blue line represents a guess at the optimal number of iterations which achieves a balance between overfitting and predictive power.  Another parameter useful for optimization is known as the learning rate which generally specifies the rate of the model's convergence.  A lower learning rate is regarded as a protection against overfitting at the expense of computation.  After randomly splitting the 2014-2015 lineup data into a training and testing set, I found that $\textit{RAPM}$ derived win projections had a mean squared error of \Sexpr{round(rapm_mse, digits = 3)} versus the model's mean squared error $\Sexpr{round(gbm_mse, digits = 3)}$ on the training set.
\begin{equation}
\mathbf{MSE}_{RAPM}=\Sexpr{round(rapm_mse, digits = 3)}\left |  \right |\mathbf{MSE}_{Model}=\Sexpr{round(gbm_mse, digits = 3)}
\end{equation}

\paragraph
\\
The real test of the model's predictive power, is using it to predict lineup winning percentages in future seasons.  To do this I predicted winning percentages on a test dataset of $n=978$ \footnote{Data gathered from Basketball-Reference} lineups randomly selected from the original dataset containing information on a total of $4889$ different lineups.  As a result I found that $\textit{RAPM}$ derived win projections had a mean squared error of $\Sexpr{round(rapm_mse, digits = 3)}$ versus the model's mean squared error of $\Sexpr{round(gbm_mse, digits = 3)}$.  This indicates improved performance versus the training set an gives and indication to the model's generalizability.
\begin{equation}
\mathbf{MSE}_{RAPM}=\Sexpr{round(rapm_mse, digits = 3)}\left |  \right |\mathbf{MSE}_{Model}=\Sexpr{round(gbm_mse, digits = 3)}
\end{equation}

\newpage
\paragraph
\\
The results of the model are also as extensible as the results from  $\textit{RAPM}$.  Just as we can use $\textit{RAPM}$ to find the the projected wins from the inclusion of a player into a lineup, we can do the same with modeling.  First we project the wins from a lineup minus the player using the model to predict winning percentages. This is then multiplied by the $\%$ of minutes and $\%$ of games the lineup is expected to play.  These steps are then repeated with the player included in the lineup and we end up with a different win projection.  The difference between the two is the number of wins a player brings to that lineup.  The advantage and disadvantage of $\textit{RAPM}$ is that it presents a universal ranking while the modeling approach is dependent on the context of a player.


\paragraph
\\
Although the differences in error are rather small they are still interesting since there is much more possible optimization.  Greater feature selection and parameter selection could improve the predictive accuracy.  The model is only fit on data from one preceding season and while more historical data could improve the model I believe that there would be diminishing returns in this regard.  This is also without mentioning the different modeling approaches that would allow player evaluators to choose the model that give them the best results.  Although plus/minus metrics are great for reference, I believe that it is better to project performance through modeling than trying to create a somewhat arbitrary ranking of players.


\end{document}